# Session Log: 2026-02-03
**Focus:** Mixed Precision Training (FP16/BF16 Autocast) & Transformer Benchmarking

## Key Concepts Discussed

### 1. Autocast Fundamentals
- **核心机制**: `torch.autocast` **不改变**参数存储类型（始终 FP32），只改变**运算中间结果**
- **黑白名单**: GPU 和 CPU 维护**两套独立的名单**（LayerNorm 在 GPU 是黑名单，在 CPU 可能不是）
- **白名单操作** (Linear, ReLU): 输出 FP16/BF16
- **黑名单操作** (LayerNorm, Softmax): 输出 FP32（需要高精度统计计算）

### 2. Precision Format Trade-offs
- **FP16**: 动态范围小（±65,504），容易下溢，但尾数精度较高（10-bit）
- **BF16**: 动态范围与 FP32 相同（±3.4×10³⁸），尾数精度低（7-bit），累加误差累积更明显
- **LayerNorm 敏感性**: 方差计算 $(x-\mu)^2$ 容易下溢，除法容易损失精度

### 3. Mixed Precision Training Architecture
- **Forward/Backward**: 在 autocast 内进行（利用 Tensor Cores 加速）
- **Optimizer.step()**: **必须在 autocast 外**（Master Weights 始终保持 FP32）
- **Warmup 策略**: 必须预热**目标精度模式**（FP32 warmup 后测 BF16 会导致 JIT 编译开销）

### 4. Benchmarking Insights
- **加速比趋势**: BF16 提供 2-3x 加速，但**不随模型大小单调增加**
- **Memory-bound vs Compute-bound**:
  - 小模型: 更多 LayerNorm/elementwise 操作 → 更 memory-bound → BF16 加速更明显（3x）
  - 大模型: GEMM 主导 → 更 compute-bound → 加速比趋于饱和（2.2x）
- **显存节省**: BF16 允许更大 batch/context（2.7B 模型在 context=1024 不 OOM）

### 5. Engineering Best Practices
- 使用 `nullcontext` 实现条件性 autocast
- 避免在 autocast 内测量时间（context 进入/退出开销）
- 区分 "logits"（softmax 前）和 "probabilities"（softmax 后）

## Technical Breakthroughs

- **Realized that autocast dtype behavior is device-specific**: CPU 和 GPU 的黑白名单不同，导致 LayerNorm 输出 dtype 差异
- **Discovered why BF16 doesn't always help large models more**: 2.7B 模型层数更少（32 vs 48），GEMM 占比更高，memory-bound 操作比例下降
- **Understood the relationship between arithmetic intensity and speedup**: Small models benefit more from BF16 because they spend more time on memory-bound ops (LayerNorm, residuals)

## Unresolved Issues
- [ ] 需要进一步验证 BF16 在 TPU 上的 LayerNorm 行为（JAX 允许 BF16 LayerNorm，PyTorch 保守使用 FP32）
- [ ] 需要探索 GradScaler 在 BF16 训练中的必要性（BF16 不需要 loss scaling，但 FP16 需要）
- [ ] 需要理解 BF16 累加误差在长时间训练中的累积效应（AdamW 的 momentum 和 variance 是否会受 BF16 精度影响）

## Action Items & Review Queue
- [ ] 阅读 PyTorch Autocast 源码，理解黑白名单的注册机制
- [ ] 重新运行实验，添加 Nsight Compute 分析，确认 GEMM 占比与模型大小的关系
- [ ] 对比 FP16 vs BF16 在更大 batch size 下的表现（验证 compute-bound 场景的加速比）
- [ ] 研究 FlashAttention 的 BF16 支持及其对整体加速的影响
