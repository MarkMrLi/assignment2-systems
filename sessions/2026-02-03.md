# Session Log: 2026-02-03
**Focus:** Mixed Precision Training (FP16/BF16 Autocast) & Transformer Benchmarking

## Key Concepts Discussed

### 1. Autocast Fundamentals
- **核心机制**: `torch.autocast` **不改变**参数存储类型（始终 FP32），只改变**运算中间结果**
- **黑白名单**: GPU 和 CPU 维护**两套独立的名单**（LayerNorm 在 GPU 是黑名单，在 CPU 可能不是）
- **白名单操作** (Linear, ReLU): 输出 FP16/BF16
- **黑名单操作** (LayerNorm, Softmax): 输出 FP32（需要高精度统计计算）

### 2. Precision Format Trade-offs
- **FP16**: 动态范围小（±65,504），容易下溢，但尾数精度较高（10-bit）
- **BF16**: 动态范围与 FP32 相同（±3.4×10³⁸），尾数精度低（7-bit），累加误差累积更明显
- **LayerNorm 敏感性**: 方差计算 $(x-\mu)^2$ 容易下溢，除法容易损失精度

### 3. Mixed Precision Training Architecture
- **Forward/Backward**: 在 autocast 内进行（利用 Tensor Cores 加速）
- **Optimizer.step()**: **必须在 autocast 外**（Master Weights 始终保持 FP32）
- **Warmup 策略**: 必须预热**目标精度模式**（FP32 warmup 后测 BF16 会导致 JIT 编译开销）

### 4. Benchmarking Insights (Corrected)
- **修正后的加速比趋势** (context_length=128):
  - Small: FP32 51.46ms → BF16 53.65ms (**0.96x**, 反而变慢！)
  - Medium: FP32 93.50ms → BF16 98.61ms (**0.95x**, 反而变慢！)
  - Large: FP32 184.01ms → BF16 145.66ms (**1.26x**, 轻微加速)
  - xl: FP32 325.25ms → BF16 193.14ms (**1.68x**, 明显加速)
  - 2.7B: FP32 500.17ms → BF16 229.67ms (**2.18x**, 最大加速)
  
- **关键发现**: BF16 **加速比随模型增大而增加**，不是之前误解的"不单调"
  - 小模型: Tensor Core 启动开销主导，cast 操作无法 amortize → 减速
  - 大模型: GEMM 足够大，充分使用 Tensor Cores → 显著加速
  
- **显存节省**: BF16 允许更大 batch/context（2.7B 模型在 context=1024 不 OOM）

### 5. Engineering Best Practices
- 使用 `nullcontext` 实现条件性 autocast
- 避免在 autocast 内测量时间（context 进入/退出开销）
- 区分 "logits"（softmax 前）和 "probabilities"（softmax 后）

## Technical Breakthroughs

- **Realized that autocast dtype behavior is device-specific**: CPU 和 GPU 的黑白名单不同，导致 LayerNorm 输出 dtype 差异
- **Discovered BF16 speedup scales with model size**: Small models slow down (0.96x), large models speed up (2.18x) - contrary to initial "memory-bound" intuition
- **Understood Tensor Core overhead dominance**: Small models can't amortize Tensor Core launch costs and FP32→BF16 cast overhead, making BF16 slower for them
- **Corrected the "memory-bound" misconception**: BF16 doesn't help small models more; it helps large models more because they have sufficient GEMM volume to utilize Tensor Cores effectively

## Unresolved Issues
- [ ] 需要进一步验证 BF16 在 TPU 上的 LayerNorm 行为（JAX 允许 BF16 LayerNorm，PyTorch 保守使用 FP32）
- [ ] 需要探索 GradScaler 在 BF16 训练中的必要性（BF16 不需要 loss scaling，但 FP16 需要）
- [ ] 需要理解 BF16 累加误差在长时间训练中的累积效应（AdamW 的 momentum 和 variance 是否会受 BF16 精度影响）
- [ ] 需要量化 Tensor Core 启动开销在小模型中的具体占比（使用 Nsight Compute）

## Action Items & Review Queue
- [ ] 阅读 PyTorch Autocast 源码，理解黑白名单的注册机制
- [ ] 使用 Nsight Compute 分析 Small vs Large 模型的 Tensor Core 利用率差异
- [ ] 对比 FP16 vs BF16 在更大 batch size 下的表现（验证 compute-bound 场景的加速比）
- [ ] 研究 FlashAttention 的 BF16 支持及其对整体加速的影响
- [ ] 探索 kernel fusion 优化（减少 cast 开销）对小模型 BF16 性能的影响
