# Session Log: 2026-01-31
**Focus:** Nsys Profiling & CUDA Kernel Analysis for Transformer Training

## Key Concepts Discussed

### 1. Nsys Profiling Workflow
- Used `torch.cuda.nvtx.range()` to annotate Forward/Backward/Optimizer phases
- Filtered kernels by NVTX range in Nsight Systems GUI
- Extracted kernel-level timing from "CUDA GPU Kernel Summary"

### 2. GEMM Kernel Analysis
- **Forward**: Uses `tn_n` pattern (left matrix transposed) → corresponds to `Y = X @ W^T`
- **Backward**: Uses `nn_n` pattern (no transpose) → corresponds to `dW = X^T @ dY`
- **Tile sizes**: Forward uses 128×128 (larger), Backward uses 64×64 (smaller due to register pressure)

### 3. Memory-bound vs Compute-bound
- Small models: Bwd/Fwd ratio ~1.1x (memory-bound, both limited by HBM bandwidth)
- Large models: Bwd/Fwd ratio approaches 2x (compute-bound, FLOPs become dominant)
- Context length increase: Forward stays flat (memory-bound), Backward scales up (compute-bound)

### 4. Softmax vs GEMM Analysis
- FLOPs difference: ~100-1000x (GEMM >> Softmax)
- Runtime difference: only ~8.5x (Softmax is memory-bound)
- End-to-end latency: Softmax ~400μs including memory overhead, nearly matching GEMM
- **Insight**: This motivates FlashAttention's kernel fusion approach

## Technical Breakthroughs

1. **Understood cuBLAS kernel naming convention**: `sm80_xmma_gemm_f32f32_tn_n_tilesize128x128x8`
   - `sm80` = GPU architecture (Ampere)
   - `tn_n` = transpose pattern
   - `tilesize` = SRAM tile dimensions

2. **Validated kernel instance counts**: 217 instances for Large model matches 36 layers × 6 GEMMs/layer

3. **Identified why FLOPs ≠ Runtime**: Memory-bound operations (Softmax, elementwise) have low arithmetic intensity

## Unresolved Issues

- [ ] Need to verify exact Softmax kernel breakdown (which sub-kernels contribute to 400μs)
- [ ] Compare profiling results with FlashAttention implementation
- [ ] Investigate why XL and 2.7B models OOM at context_length=1024

## Commands Used

```bash
# Nsys profiling
nsys profile -o profile_output --trace=cuda,nvtx python benchmarking_script.py

# View in GUI
nsys-ui profile_output.nsys-rep
```
