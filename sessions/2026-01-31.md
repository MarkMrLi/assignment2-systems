# Session Log: 2026-01-31
**Focus:** Nsys Profiling & CUDA Kernel Analysis for Transformer Training, Mixed Precision Training

## Key Concepts Discussed

### 1. Nsys Profiling Workflow
- Used `torch.cuda.nvtx.range()` to annotate Forward/Backward/Optimizer phases
- Filtered kernels by NVTX range in Nsight Systems GUI
- Extracted kernel-level timing from "CUDA GPU Kernel Summary"

### 2. GEMM Kernel Analysis
- **Forward**: Uses `tn_n` pattern (left matrix transposed) → corresponds to `Y = X @ W^T`
- **Backward**: Uses `nn_n` pattern (no transpose) → corresponds to `dW = X^T @ dY`
- **Tile sizes**: Forward uses 128×128 (larger), Backward uses 64×64 (smaller due to register pressure)

### 3. Memory-bound vs Compute-bound
- Small models: Bwd/Fwd ratio ~1.1x (memory-bound, both limited by HBM bandwidth)
- Large models: Bwd/Fwd ratio approaches 2x (compute-bound, FLOPs become dominant)
- Context length increase: Forward stays flat (memory-bound), Backward scales up (compute-bound)

### 4. Softmax vs GEMM Analysis
- FLOPs difference: ~100-1000x (GEMM >> Softmax)
- Runtime difference: only ~8.5x (Softmax is memory-bound)
- End-to-end latency: Softmax ~400μs including memory overhead, nearly matching GEMM
- **Insight**: This motivates FlashAttention's kernel fusion approach

### 5. Mixed Precision Training: Dynamic Range vs Precision (NEW)
- **Dynamic Range**: Determined by exponent bits, controls min/max representable values
- **Precision**: Determined by mantissa bits, controls relative accuracy between values

| Format | Exponent | Mantissa | Dynamic Range | Precision |
|--------|----------|----------|---------------|-----------|
| FP32   | 8 bits   | 23 bits  | ~10^-38 to 10^38 | ~7 decimal digits |
| FP16   | 5 bits   | 10 bits  | ~6×10^-8 to 65504 | ~3-4 decimal digits |
| BF16   | 8 bits   | 7 bits   | Same as FP32 | ~2-3 decimal digits |

### 6. Why FP16 is Unstable (NEW)
- **Gradient underflow**: Small gradients (e.g., 1e-7) flush to zero due to limited dynamic range
- **Activation overflow**: Large values (e.g., QK^T before scaling) exceed 65504 → NaN
- **Solution**: Loss Scaling shifts gradient values to avoid the "dead zone"

### 7. Why BF16 is Stable but Affects Performance (NEW)
- **Stable**: Same dynamic range as FP32 (8-bit exponent) → no overflow/underflow
- **Performance impact**: Low precision (7-bit mantissa) → small updates get "swallowed" by large weights
- Example: `1.0 - 0.00001` in BF16 → relative change 0.001% < BF16 precision ~0.78% → truncated to 1.0

### 8. Attention Overflow Mechanism (NEW)
- `QK^T` accumulates `d_k` products before scaling
- If Q, K elements are large, `exp(QK^T)` in softmax can overflow FP16
- `ln(65504) ≈ 11`, so `exp(x)` overflows when x > 11
- **Solution**: Safe softmax (subtract max before exp), `/ sqrt(d_k)` scaling

## Technical Breakthroughs

1. **Understood cuBLAS kernel naming convention**: `sm80_xmma_gemm_f32f32_tn_n_tilesize128x128x8`
   - `sm80` = GPU architecture (Ampere)
   - `tn_n` = transpose pattern
   - `tilesize` = SRAM tile dimensions

2. **Validated kernel instance counts**: 217 instances for Large model matches 36 layers × 6 GEMMs/layer

3. **Identified why FLOPs ≠ Runtime**: Memory-bound operations (Softmax, elementwise) have low arithmetic intensity

4. **Clarified Dynamic Range vs Precision distinction** (NEW):
   - Dynamic range issue → values become 0 or inf/NaN
   - Precision issue → large numbers "swallow" small updates (truncation)

5. **Loss Scaling insight** (NEW): "Shifts gradient values to avoid FP16's dead zone" - does not change dynamic range, just repositions values into representable range

## Unresolved Issues

- [ ] Need to verify exact Softmax kernel breakdown (which sub-kernels contribute to 400μs)
- [ ] Compare profiling results with FlashAttention implementation
- [ ] Investigate why XL and 2.7B models OOM at context_length=1024
- [ ] Implement safe softmax and verify numerical stability improvement (NEW)
- [ ] Explore mixed precision training with dynamic loss scaling (NEW)

## Commands Used

```bash
# Nsys profiling
nsys profile -o profile_output --trace=cuda,nvtx python benchmarking_script.py

# View in GUI
nsys-ui profile_output.nsys-rep
```
