# Session Log: 2026-01-30
**Focus:** GPU Memory Management, Submitit Concurrency, CUDA Resource Contention

## Key Concepts Discussed

### 1. CUDA Memory Allocator Behavior
- **Socratic Path**: Started with "OOM happens when model is 'too big'?" → led to discovery of PyTorch CUDA caching allocator
- **Insight**: `del` / `model = None` doesn't return memory to OS immediately; it's cached for reuse within the same process
- **Impact**: This is usually a performance optimization, but becomes a liability when multiple processes compete for the same GPU

### 2. Submitit + LocalExecutor Execution Model
- **Discovery**: User's environment has no SLURM, so `submitit.AutoExecutor` falls back to `LocalExecutor`
- **Behavior**: `with executor.batch():` submits all jobs immediately without waiting
- **Problem**: 20 jobs (4 context_lengths × 5 models) submitted at once, with 4 jobs per GPU competing for memory

### 3. Resource Contention vs Model Size
- **Counter-intuitive Finding**: OOM pattern didn't correlate with model size
  - xl model succeeded at context=128 but failed at 256
  - 2.7B model succeeded at context=256 but failed at 128
- **Root Cause**: "Whoever grabs memory first wins" - OOM is about timing and allocation order, not absolute memory requirements

### 4. Concurrency Control Strategy
- **Solution Pattern**: Move `with executor.batch()` inside the outer loop to control batch size
- **Result**: 5 GPUs × 1 job each = 5 concurrent jobs max, instead of 20
- **Synchronization Point**: `job.result()` acts as natural barrier before next batch

## Technical Breakthroughs

- **Realized that OOM diagnosis requires checking concurrency patterns, not just model size**
- **Understood that `CUDA_VISIBLE_DEVICES` setting must happen BEFORE `import torch` in subprocess**
- **Discovered that `executor.batch()` + `job.result()` can be used as manual resource throttling mechanism**

## Unresolved Issues

- [ ] Need to verify if `torch.cuda.empty_cache()` is necessary between batches for user's specific case
- [ ] Could explore `multiprocessing.Pool` as alternative to submitit for single-node benchmarks
- [ ] Future: consider installing SLURM for true multi-user resource isolation on single node

## Action Items for Next Session

1. Run benchmark with fixed concurrency control to verify OOM is resolved
2. If OOM persists, investigate `torch.cuda.empty_cache()` placement
3. Document the "batch-within-loop" pattern as reusable template for future benchmarks
