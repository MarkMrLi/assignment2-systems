# Session Log: 2026-02-04
**Focus:** PyTorch CUDA Memory Profiling (memory_viz), Inference vs Training Memory, Mixed Precision Memory Artifacts

## Key Concepts Discussed
- Memory snapshots: `torch.cuda.memory._record_memory_history()` + `torch.cuda.memory._dump_snapshot()` + analysis in https://pytorch.org/memory_viz.
- Inference forward vs training forward: `torch.inference_mode()`/`torch.no_grad()` prevents autograd graph construction and activation saving.
- Full training step memory: forward + backward + optimizer step; baseline often dominated by parameters + AdamW states (m/v).
- Timeline interpretation: “Active memory timeline” shows phase structure; “largest allocations” require good recording coverage and stack capture.

## Technical Breakthroughs
- Diagnosed BF16 inference “staircase growth” as `torch.autocast` weight-cast cache; confirmed by disabling cache (`cache_enabled=False`) which removed the staircase pattern.
- Understood why FP32 vs BF16 peak memory can look similar at short context lengths: parameter + optimizer-state dominance can mask activation savings; BF16 may add overhead via casts/caches.
- Computed residual-stream activation tensor size for 2.7B config: for `B=4, T=512, d_model=2560`, FP32 activations are ~20.0 MiB.
- Identified largest allocations (~100 MiB, ~25 MiB) as parameter copies during `model.to(device)` via stack traces (`_to_copy/empty_strided_cuda`), not forward activations.

## Unresolved Issues
- Need to extract and tabulate exact peak “Active memory” for context lengths 128/256/512 for (forward-only, full-step) × (FP32, BF16) to complete deliverables (b) and (c).
