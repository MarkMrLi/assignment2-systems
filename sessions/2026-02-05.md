# Session Log: 2026-02-05
**Focus:** FlashAttention-2 Benchmarking & Memory Analysis

## Key Concepts Discussed

### 1. Naïve PyTorch Attention Implementation
- Implemented `scaled_dot_product_attention()` following the FlashAttention-2 paper's formulation:
  $$Attention(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
- Key observation: This implementation materializes the full $N \times N$ attention score matrix

### 2. Memory Profiling Methodology
- Discovered critical distinction between:
  - `torch.cuda.memory_allocated()`: Current allocation
  - `torch.cuda.max_memory_allocated()`: Peak allocation since reset
- Identified measurement timing issues:
  - Forward pass memory allocated **during** computation, not after
  - Solution: Use `torch.cuda.reset_peak_memory_stats()` before forward pass

### 3. GPU Memory Composition Analysis
Through step-by-step memory tracing, decomposed peak memory into components:

| Component | Size | Scaling |
|-----------|------|---------|
| Q, K, V tensors | ~0.125 MB | O(seq_len × d_model) |
| Attention scores (S) | ~2 MB | O(seq_len²) |
| Softmax output (P) | ~2 MB | O(seq_len²) |
| Gradients (dQ, dK, dV) | ~0.375 MB | O(seq_len × d_model) |
| cuBLAS workspace | 64 MB (fwd) + 64 MB (bwd) | Fixed overhead |
| **Total (seq_len=256)** | **~72 MB** | - |

### 4. PyTorch Autograd Internals
- **Leaf nodes** (Q, K, V): `requires_grad=True`, persist `.grad`
- **Intermediate tensors** (S, P, O): Created by operations, no persistent `.grad`
- **Key insight**: PyTorch uses **lazy evaluation** - only computes and stores what's needed for backward
- **Memory reuse**: dO can be computed and immediately released before computing dP

### 5. Scaling Laws
- **Forward time**: ~O(seq_len²) - dominated by QK^T and PV matmuls
- **Activation memory**: ~O(seq_len²) - attention scores are quadratic
- **Parameter gradients**: ~O(seq_len × d_model) - linear scaling

### 6. FlashAttention Motivation
- **Problem**: O(N²) activation memory prevents scaling to long sequences
- **seq_len=16384** requires ~16 GB just for attention scores (FP32, batch=8)
- **Solution approaches**:
  - Gradient checkpointing: Recompute activations during backward (trade compute for memory)
  - FlashAttention tiling: Process attention in blocks, keep O(N × block_size) in HBM
  - Kernel fusion: Fuse QK^T + softmax + PV into single kernel

## Technical Breakthroughs

1. **Precise Memory Decomposition**: Successfully decomposed 72.77 MB peak memory into cuBLAS workspace (64 MB), QKV+gradients (~4.5 MB), and attention scores (~4 MB)

2. **Understanding CUDA Overhead**: Realized cuBLAS workspace (64 MB each for fwd/bwd) is fixed overhead, dominant at small seq_len but negligible at large seq_len

3. **OOM Boundary Estimation**: With 98 GB GPU memory, theoretical OOM occurs between seq_len=16384 (33 GB used) and seq_len=32768 (~130 GB theoretical)

## Unresolved Issues

1. **Detailed Activation Gradient Tracking**: The backward pass may reuse memory (dP computed in-place), exact allocation pattern needs verification with PyTorch source
2. **FlashAttention Implementation**: Next step is implementing tiled attention following FlashAttention-2 algorithm
