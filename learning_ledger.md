# CS336 Learning Ledger
**Last Updated:** 2026-02-03

---

## ðŸ§  Knowledge Graph (Mastery Levels)

### ðŸŸ¢ Mastered
- PyTorch `zero_grad()` æœºåˆ¶ä¸Žæ¢¯åº¦ç´¯ç§¯
- Forward/Backward Pass è¾¹ç•Œå®šä¹‰ï¼ˆè®¡ç®—å›¾æž„å»º vs æ¶ˆè´¹ï¼‰
- `torch.cuda.synchronize()` ç”¨äºŽå‡†ç¡® GPU æ—¶é—´æµ‹é‡
- å•æœºå¤šå¡çŽ¯å¢ƒä¸‹ `torch.cuda.set_device()` ä½¿ç”¨
- PyTorch CUDA memory caching allocator behavior
- Submitit LocalExecutor execution model and batch submission semantics
- GPU resource contention diagnosis methodology
- Concurrency control via loop restructuring
- **Benchmarking fundamentals**: Warmup importance for stable measurements
- **NVTX annotation**: Using `torch.cuda.nvtx.range()` for profiling phase marking
- **Memory-bound vs Compute-bound**: Small models memory-bound (Bwd/Fwd ~1.1x), large models compute-bound (~2x)
- **GEMM kernel naming**: cuBLAS convention (`tn_n`=left transpose, `nn_n`=no transpose, tile sizes)
- **Dynamic Range vs Precision distinction**: Exponent bits â†’ range, Mantissa bits â†’ precision
- **FP16/BF16/FP32 trade-offs**: BF16 stable (same range as FP32), FP16 precise but unstable
- **Loss Scaling mechanism**: Shifts gradients to avoid FP16's representational dead zone
- **FP16 Spacing/ULP**: Gap between representable FP16 numbers grows exponentially with magnitude
- **Round-to-Nearest Behavior**: IEEE 754 rounding and its impact on accumulation accuracy
- **Mixed Precision Accumulation Pattern**: Accumulator must use FP32 even when operands are FP16 - this is Master Weights core principle
- **Autocast Mechanism**: å‚æ•°å­˜å‚¨ç±»åž‹ vs è¿ç®—ç²¾åº¦çš„åŒºåˆ«ï¼Œé»‘ç™½åå•æ¦‚å¿µ
- **Device-Specific Autocast Behavior**: CPU vs GPU é»‘ç™½åå•å·®å¼‚ï¼ˆLayerNorm åœ¨ GPU é»‘åå•ï¼ŒCPU å¯èƒ½ä¸æ˜¯ï¼‰
- **BF16 Trade-offs**: åŠ¨æ€èŒƒå›´è¶³å¤Ÿä½†ç´¯åŠ ç²¾åº¦è¾ƒä½Žï¼ˆ7-bit å°¾æ•° vs FP16 çš„ 10-bitï¼‰
- **LayerNorm Precision Sensitivity**: æ–¹å·®è®¡ç®—å®¹æ˜“ä¸‹æº¢ï¼Œé™¤æ³•å®¹æ˜“æŸå¤±ç²¾åº¦
- **Mixed Precision Training Flow**: Forward/Backward in autocast, Optimizer outside (Master Weights)
- **Conditional Autocast**: Using `nullcontext` for no-op precision switching
- **Warmup Strategy**: Must match target precision (FP32 warmup â†’ BF16 measure = JIT overhead contamination)

### ðŸŸ¡ Developing
- Slurm + CUDA_VISIBLE_DEVICES äº¤äº’æœºåˆ¶
- submitit å¤šè¿›ç¨‹ä»»åŠ¡è°ƒåº¦
- Distributed training setup (SLURM vs non-SLURM environments)
- Multi-GPU benchmarking best practices
- CUDA context management across subprocess boundaries
- Memory profiling and OOM debugging techniques
- **Nsys profiling workflow**: Can use GUI effectively, need more practice with CLI stats extraction
- **Kernel-level analysis**: Can identify top kernels, building intuition for optimization
- **Tile size trade-offs**: 128Ã—128 for high arithmetic intensity, 64Ã—64 when register pressure high
- **Arithmetic Intensity**: Can explain concept, need practice calculating for specific ops
- **Mixed Precision Training**: Hands-on implementation with BF16 benchmark completed
- **Speedup Analysis**: Understanding why BF16 speedup plateaus and doesn't scale with model size
- **Accumulation Error Visualization**: Can predict error patterns, need to implement visualization tools

### ðŸ”´ Blind Spots
- Slurm é›†ç¾¤çŽ¯å¢ƒä¸‹çš„ GPU åˆ†é…ä¸Žéš”ç¦»ç­–ç•¥
- **FlashAttention implementation**: Understand motivation (Softmax memory overhead), haven't implemented
- Quantization (FP8/FP4) memory savings calculation
- Gradient checkpointing trade-offs
- Tensor Parallelism vs Pipeline Parallelism decision criteria
- AdamW optimizer state memory overhead calculation
- **Tensor Core utilization measurement**: Know they accelerate GEMM, don't know how to measure
- **Safe Softmax implementation**: Know the principle (subtract max), need to verify in code
- **TPU/Mixed Precision**: Google TPU's BF16 strategy vs NVIDIA GPU differences
- **Stochastic Rounding**: Alternative rounding strategies for training stability
- **Error Propagation in Deep Networks**: How numerical errors compound across transformer layers
- **GradScaler Mechanics**: FP16 needs loss scaling, BF16 doesn't - understand implementation details
- **Long-term Training Stability**: BF16 accumulation error impact over 100k+ steps

---

## ðŸ“‰ Action Items & Review Queue

### Immediate (Next Session)
- [ ] Implement FlashAttention and compare profiling results
- [ ] Investigate OOM causes for XL/2.7B models at context_length=1024
- [ ] Implement safe softmax and test numerical stability
- [ ] Compare BF16 vs FP16 accumulation experiment (0.01 Ã— 1000 test)
- [ ] Read PyTorch Autocast æºç ï¼Œç†è§£é»‘ç™½åå•çš„æ³¨å†Œæœºåˆ¶ (`torch/_autocast_utils.py`)
- [ ] Add Nsight Compute analysis to benchmarkï¼Œé‡‡é›†ç®—æœ¯å¼ºåº¦å’Œå¸¦å®½åˆ©ç”¨çŽ‡æŒ‡æ ‡

### Short-term (This Week)
- [ ] äº†è§£ Slurm `--gres=gpu:N` ä¸Ž CUDA_VISIBLE_DEVICES çš„æ˜ å°„å…³ç³»
- [ ] Review FlashAttention paper - understand the SRAM vs HBM movement
- [ ] Implement memory profiling using `torch.cuda.memory_summary()`
- [ ] Learn `nsys stats` CLI commands for automated data extraction
- [ ] Practice calculating arithmetic intensity for Attention operations
- [ ] Implement mixed precision training with dynamic loss scaling
- [ ] Compare FP32 vs BF16 vs FP16 training convergence on small model
- [ ] Cross-Platform Test: åœ¨ CPU ä¸Šå¤çŽ° BF16 å®žéªŒï¼ŒéªŒè¯ LayerNorm è¾“å‡º dtype å·®å¼‚

### Long-term (Before Assignment Due)
- [ ] Compare benchmark results with theoretical FLOPs calculations
- [ ] Explore mixed precision training impact on memory and speed
- [ ] Study Tensor Core architecture and how to maximize utilization
- [ ] Implement custom BF16 LayerNorm and test numerical stability
- [ ] Compare with JAX/Flax BF16 strategy

---

## ðŸ“š Session History

| Date | Topic | Key Outcome |
|------|-------|-------------|
| 2026-01-30 | Benchmarking Code Review | æŽŒæ¡ Forward/Backward è¾¹ç•Œã€GPU è®¾å¤‡é€‰æ‹©æœºåˆ¶ |
| 2026-01-30 | OOM Diagnosis & Concurrency Fix | Diagnosed submitit OOM by restructuring batch submission pattern |
| 2026-01-31 | Nsys Profiling & Kernel Analysis | Mastered GEMM kernel naming, understood memory-bound vs compute-bound, analyzed Softmax overhead motivating FlashAttention |
| 2026-01-31 | Mixed Precision Training | Clarified Dynamic Range vs Precision, understood FP16/BF16/FP32 trade-offs, Loss Scaling mechanism |
| 2026-02-02 | Mixed Precision Accumulation | Discovered FP16 spacing impact on accumulation, understood Master Weights design rationale, verified Round-to-Nearest behavior |
| **2026-02-03** | **BF16 Autocast & Benchmarking** | **Discovered CPU vs GPU autocast behavior differences, implemented BF16 benchmarking, analyzed speedup trends (small models 3x, large models 2.2x), understood why optimizer must be outside autocast** |

---

## ðŸŽ¯ Course Goals (Assignment 2 - Systems)
- [x] Build benchmarking infrastructure with multiple model sizes
- [x] Profile Forward/Backward with Nsys, analyze kernel distribution
- [x] Implement mixed precision training with BF16 autocast
- [ ] Implement memory-efficient attention (FlashAttention)
- [ ] Implement distributed training strategies
- [ ] Optimize training throughput

---

## ðŸ’¡ Key Insights Archive

### 2026-02-03: BF16 Speedup Plateau
> "BF16 achieves ~3x speedup for Small models (memory-bound) but only ~2.2x for 2.7B model (compute-bound).
> Speedup doesn't scale with parameter count because 2.7B has fewer layers (32 vs 48), meaning higher GEMM percentage.
> This confirms: arithmetic intensity, not raw parameters, determines mixed precision gains."

### 2026-02-03: Memory Savings > Speed Gains
> "BF16's primary benefit is preventing OOM (XL model runs at context_length=1024 in BF16 but OOM in FP32).
> The ~2-3x speedup is secondary to the ability to train larger models/batches."

### 2026-02-03: Autocast is Device-Specific
> "LayerNorm outputs float32 on GPU (blacklisted) but bfloat16 on CPU (not blacklisted).
> PyTorch maintains separate allow/deny lists per device backend. This can cause cross-platform inconsistencies."

### 2026-02-03: Warmup Must Match Target Precision
> "If you FP32-warmup then BF16-measure, the first BF16 iteration includes JIT kernel compilation overhead.
> Fair comparison requires separate warmup for each precision mode."

### 2026-02-03: Optimizer Outside Autocast
> "Master Weights require optimizer.step() to remain in FP32. Placing it inside autocast doesn't break anything
> (autocast doesn't affect optimizer), but semantically it belongs outside - autocast is for compute, not parameter update."

### 2026-02-03: Small Models More Memory-Bound
> "Contrary to intuition, Small models show higher BF16 speedup (3x) than Large (2.9x) or 2.7B (2.2x).
> Reason: Small models spend more time on memory-bound ops (LayerNorm, residuals, data movement).
> BF16 halves HBM traffic, benefiting memory-bound workloads most."

### 2026-02-02: FP16 Accumulation Error is Non-uniform
> "FP16 spacing increases with magnitude. Adding 0.01 to 16.0 in FP16 yields 16.015625 
> (spacing=0.0156), while adding to 1.0 yields 1.009765625. The error direction 
> varies by range - unpredictable and unidirectional. This is why pure FP16 
> gradient accumulation fails."

### 2026-02-02: Master Weights Principle
> "Even when gradients are computed and stored as FP16, the accumulation buffer 
> must be FP32. This eliminates the spacing problem entirely - FP32's spacing 
> at typical gradient magnitudes is negligible. This is the numerical foundation 
> for Master Weights in mixed precision training."

### 2026-02-02: FP16 Representation of 0.01
> "torch.tensor(0.01, dtype=torch.float16) stores 0.01000213623046875, not 0.01. 
> The representation error starts immediately, not just at large values. This 
> explains why even early-stage FP16 accumulation shows systematic bias."

### 2026-01-31: Dynamic Range vs Precision
> "Dynamic range (exponent bits) determines if a value can be represented at all.
> Precision (mantissa bits) determines if two close values can be distinguished.
> FP16 has narrow range but decent precision; BF16 has full range but low precision.
> This is why BF16 is 'stable but affects performance' - no overflow, but small updates get swallowed."

### 2026-01-31: Loss Scaling Essence
> "Loss Scaling doesn't change FP16's dynamic range - it shifts gradient values 
> away from the 'dead zone' (values too small to represent). 
> Like moving your data to a different floor in a building, not making the building taller."

### 2026-01-31: Attention Overflow Point
> "QK^T accumulates d_k products. With exp() in softmax, overflow occurs when QK^T > ln(65504) â‰ˆ 11.
> This is why we need: (1) / sqrt(d_k) scaling, (2) safe softmax (subtract max before exp)."

### 2026-01-31: FLOPs â‰  Runtime
> "Softmax has ~100x fewer FLOPs than Attention GEMM, but only runs ~8.5x faster. 
> End-to-end latency is nearly identical (~400Î¼s vs ~300Î¼s) due to memory overhead. 
> This is why FlashAttention fuses Softmax with Attention computation in SRAM."

### 2026-01-31: Backward uses smaller tiles
> "Backward pass uses 64Ã—64 tiles instead of Forward's 128Ã—128 because it needs 
> to store both activations and gradients, creating higher register pressure."

### 2026-01-31: Memory-bound to Compute-bound transition
> "Small models show Bwd/Fwd ratio ~1.1x (both memory-bound), while large models 
> approach theoretical 2x ratio (compute-bound). This explains why optimization 
> strategies differ by model scale."

---

*This ledger is updated at the end of each study session. Say "End Session" to generate updates.*
