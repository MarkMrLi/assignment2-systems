# CS336 Learning Ledger
**Last Updated:** 2026-01-31

---

## ðŸ§  Knowledge Graph (Mastery Levels)

### ðŸŸ¢ Mastered
- PyTorch `zero_grad()` æœºåˆ¶ä¸Žæ¢¯åº¦ç´¯ç§¯
- Forward/Backward Pass è¾¹ç•Œå®šä¹‰ï¼ˆè®¡ç®—å›¾æž„å»º vs æ¶ˆè´¹ï¼‰
- `torch.cuda.synchronize()` ç”¨äºŽå‡†ç¡® GPU æ—¶é—´æµ‹é‡
- å•æœºå¤šå¡çŽ¯å¢ƒä¸‹ `torch.cuda.set_device()` ä½¿ç”¨
- PyTorch CUDA memory caching allocator behavior
- Submitit LocalExecutor execution model and batch submission semantics
- GPU resource contention diagnosis methodology
- Concurrency control via loop restructuring
- **Benchmarking fundamentals**: Warmup importance for stable measurements *(æ–°å¢ž)*
- **NVTX annotation**: Using `torch.cuda.nvtx.range()` for profiling phase marking *(æ–°å¢ž)*
- **Memory-bound vs Compute-bound**: Small models memory-bound (Bwd/Fwd ~1.1x), large models compute-bound (~2x) *(æ–°å¢ž)*
- **GEMM kernel naming**: cuBLAS convention (`tn_n`=left transpose, `nn_n`=no transpose, tile sizes) *(æ–°å¢ž)*

### ðŸŸ¡ Developing
- Slurm + CUDA_VISIBLE_DEVICES äº¤äº’æœºåˆ¶
- submitit å¤šè¿›ç¨‹ä»»åŠ¡è°ƒåº¦
- Distributed training setup (SLURM vs non-SLURM environments)
- Multi-GPU benchmarking best practices
- CUDA context management across subprocess boundaries
- Memory profiling and OOM debugging techniques
- **Nsys profiling workflow**: Can use GUI effectively, need more practice with CLI stats extraction *(æ›´æ–°)*
- **Kernel-level analysis**: Can identify top kernels, building intuition for optimization *(æ–°å¢ž)*
- **Tile size trade-offs**: 128Ã—128 for high arithmetic intensity, 64Ã—64 when register pressure high *(æ–°å¢ž)*
- **Arithmetic Intensity**: Can explain concept, need practice calculating for specific ops *(æ–°å¢ž)*

### ðŸ”´ Blind Spots
- Slurm é›†ç¾¤çŽ¯å¢ƒä¸‹çš„ GPU åˆ†é…ä¸Žéš”ç¦»ç­–ç•¥
- **FlashAttention implementation**: Understand motivation (Softmax memory overhead), haven't implemented *(æ›´æ–°)*
- Quantization (FP8/FP4) memory savings calculation
- Gradient checkpointing trade-offs
- Tensor Parallelism vs Pipeline Parallelism decision criteria
- AdamW optimizer state memory overhead calculation
- **Tensor Core utilization measurement**: Know they accelerate GEMM, don't know how to measure *(æ–°å¢ž)*

---

## ðŸ“‰ Action Items & Review Queue

### Immediate (Next Session)
- [x] éªŒè¯ benchmarking è„šæœ¬åœ¨ 8 å¡çŽ¯å¢ƒä¸‹å¹¶è¡Œè¿è¡Œç»“æžœ
- [x] Verify the concurrency fix resolves all OOM cases in benchmark
- [x] å­¦ä¹ å¦‚ä½•ç”¨ Nsight Systems åˆ†æž NVTX æ ‡è®°çš„æ—¶é—´çº¿ *(å®Œæˆ)*
- [ ] Implement FlashAttention and compare profiling results *(æ–°å¢ž)*
- [ ] Investigate OOM causes for XL/2.7B models at context_length=1024 *(æ–°å¢ž)*

### Short-term (This Week)
- [ ] äº†è§£ Slurm `--gres=gpu:N` ä¸Ž CUDA_VISIBLE_DEVICES çš„æ˜ å°„å…³ç³»
- [ ] Review FlashAttention paper - understand the SRAM vs HBM movement
- [ ] Implement memory profiling using `torch.cuda.memory_summary()`
- [ ] Learn `nsys stats` CLI commands for automated data extraction *(æ–°å¢ž)*
- [ ] Practice calculating arithmetic intensity for Attention operations *(æ–°å¢ž)*

### Long-term (Before Assignment Due)
- [ ] Compare benchmark results with theoretical FLOPs calculations
- [ ] Explore mixed precision training impact on memory and speed
- [ ] Study Tensor Core architecture and how to maximize utilization *(æ–°å¢ž)*

---

## ðŸ“š Session History

| Date | Topic | Key Outcome |
|------|-------|-------------|
| 2026-01-30 | Benchmarking Code Review | æŽŒæ¡ Forward/Backward è¾¹ç•Œã€GPU è®¾å¤‡é€‰æ‹©æœºåˆ¶ |
| 2026-01-30 | OOM Diagnosis & Concurrency Fix | Diagnosed submitit OOM by restructuring batch submission pattern |
| **2026-01-31** | **Nsys Profiling & Kernel Analysis** | **Mastered GEMM kernel naming, understood memory-bound vs compute-bound, analyzed Softmax overhead motivating FlashAttention** |

---

## ðŸŽ¯ Course Goals (Assignment 2 - Systems)
- [x] Build benchmarking infrastructure with multiple model sizes
- [x] Profile Forward/Backward with Nsys, analyze kernel distribution *(æ–°å®Œæˆ)*
- [ ] Implement memory-efficient attention (FlashAttention)
- [ ] Implement distributed training strategies
- [ ] Optimize training throughput

---

## ðŸ’¡ Key Insights Archive

### 2026-01-31: FLOPs â‰  Runtime
> "Softmax has ~100x fewer FLOPs than Attention GEMM, but only runs ~8.5x faster. 
> End-to-end latency is nearly identical (~400Î¼s vs ~300Î¼s) due to memory overhead. 
> This is why FlashAttention fuses Softmax with Attention computation in SRAM."

### 2026-01-31: Backward uses smaller tiles
> "Backward pass uses 64Ã—64 tiles instead of Forward's 128Ã—128 because it needs 
> to store both activations and gradients, creating higher register pressure."

### 2026-01-31: Memory-bound to Compute-bound transition
> "Small models show Bwd/Fwd ratio ~1.1x (both memory-bound), while large models 
> approach theoretical 2x ratio (compute-bound). This explains why optimization 
> strategies differ by model scale."

---

*This ledger is updated at the end of each study session. Say "End Session" to generate updates.*
